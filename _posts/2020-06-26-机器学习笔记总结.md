---
layout:     post
title:      "机器学习笔记总结"
date:       2020-06-26 10:00:00
author:     "DadaX"
tags:
    - 传统机器学习
---

# 0. 学习资料

* [吴恩达机器学习-coursera：lecture&test](https://www.coursera.org/learn/machine-learning/home/welcome)
* [吴恩达机器学习-网易云课堂：video](https://study.163.com/course/courseMain.htm?courseId=1004570029)
* [吴恩达机器学习exercise-github](https://github.com/nsoojin/coursera-ml-py)
* 西瓜书

# 1. 机器学习简介

## 相关概念

机器学习的定义如下：
>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.<br>
>一个（机器学习）的程序就是可以从经验数据E中对任务T进行学习的算法，它在任务T上的性能度量P会随着对于经验数据E的学习而变得更好

由于机器学习必然利用了某些经验，它们常常**数据**的形式存在，我们称之为**数据集（dataset）**，其中每条记录称为一个**示例（instance）**。例如我们通过一个人的性别、年龄和身高预测他是否患某种常见疾病，有以下数据：

>（性别：男；年龄：18；身高：174；是否得病：否）
>（性别：女；年龄：17；身高：164；是否得病：是）
>（性别：男；年龄：20；身高：181；是否得病：是）
>（性别：女；年龄：16；身高：161；是否得病：是） ……

这组记录的集合称为一个“数据集”，每条记录称为一个样本。在记录中，关于该对象的描述型数据称为**属性**，由于属性往往有很多个——如上文的年龄，身高等，可以构成**属性向量**，这些向量张成的空间称为**属性空间**或**输入空间**。而我们的算法需要预测那个量（如本例中“是否得病”）称为**标记**，在有的数据集中存在标记，有的不存在。所有标记的集合称为**标记空间**或**输出空间**。

我们从数据中学得模型，这个过程称为**训练**，我们使用**训练数据**，学得**模型**，并要求该模型适用于新样本，称为**泛化能力**。



我们训练得到的模型称为一个**假设**，所有的假设一起构成了**假设空间**

对于训练的得到多个不同模型，我们如何选择呢？常用的方法是奥卡姆剃刀（Occam's razor）：

>奥卡姆剃刀：若有多个假设与观察一致，则选最简单的那

## 算法分类

**根据训练数据有无标记（label），我们将机器学习分为：**

* 监督学习 supervised learning
* 无监督学习 unsupervised learning
* 强化学习 reinforcement learning ……

下面我们依次对他们进行解释。

1. 监督学习：
2. 无监督学习：
3. 强化学习：

### 监督学习
**根据输出值是否连续，我们将监督学习任务分为：**

**1. 回归 regression**： 连续值输出。eg. 面积 - 房价预测

<img src="https://i.loli.net/2020/06/27/oUPs54wZ9JQFegN.png" alt="regression.png" style="zoom:30%;" />

**2. 分类 classification**： 离散值输出。eg. 肿瘤大小 - 良性/恶性

<img src="https://i.loli.net/2020/06/27/aNFcwxfSBTVXMoU.png" alt="classification.png" style="zoom:30%;" />

### 无监督学习

**聚类**——eg. 鸡尾酒会算法

[W,s,v] = svd((repmat(sum(x.\*x,1),size(x,1),1).\*x)\*x');

## 更多
* 推荐用**octave**来实现算法原型，再用其他的编程语言来开发

# 2. 模型评估与选择

## 经验误差与过拟合

* **错误率 E（error rate）：** *分类错误*的样本数占总样本数的比例
* **精度（accuracy）：**（1 - 错误率）%
* **误差（error）：**预测输出与样本真是值的差异
* **过拟合🙁** & **欠拟合** —— 要求学习适用于所有潜在样本的“普遍规律”

## 如何进行模型评估与选择——评估方法
>从**数据集D**中产生**训练集S**和**测试集T**

* 留出法（hold-out）：直接将数据集D划分为两个互斥的集合（*2/3～4/5*的样本用于训练）

	⚠️保持**数据分布**的一致性

		独立同分布 i.i.d：每个样本都是独立地从这个分布上采样获得
	
	⚠️单次留出法得到的估计结果往往不够稳定可靠，一般采用若干次随机划分、重复实验后取平均值
	
* 交叉验证法（k-fold cross validation(10)）

<img src="https://i.loli.net/2020/06/27/4vGgxdPjoApOMrw.png" alt="交叉验证.png" style="zoom: 50%;" />

* 自助法（少）

 从D中有放回的随机挑选m个样本放入**D'**做训练集，**D\D'**做测试集

 💭用于数据集小、难于有效划分训练/测试集时

>调参与最终模型

* 测试集：估计模型在实际使用时的泛化能力
* 验证集：模型选择和调参

## 性能度量 performance measure
* 回归任务最常用的性能度量：**均方误差（mean squared error）**

  
  $$
  E(f;D)=\frac{1}{m}\sum^m_{i=1}(f(x_i)-y_i)^2
  $$

* 分类问题常用性能度量：

	* 错误率与精度
	  $$
  E(f;D)=\frac{1}{m}\sum^m_{i=1}\mathbb I(f(x_i)\ne y_i)
	  $$
	  
	* 查准率（precision）、查全率(recall)与F1
	
	<img src="https://i.loli.net/2020/06/27/Zt7aQn1TsMcYXhg.png" alt="P&amp;R.png" style="zoom:33%;" />
	
	* 那么如何根据P和R判断学习期的性能呢？
	
	  ![PR曲线.png](https://i.loli.net/2020/06/27/GVCrBjNkAMPaQ15.png)
	
	     * 根据曲线：C曲线被A全包住，A优于C
	     * 曲线下面积大小：面积大的好
	
	  
	
	  ⬆️不易估算
	
	  
	
	  * 平衡点（break even point, BEP）:P = R, 大的好，A优于B
	    	
	
	  * **F1**
	    $$
	        F1 = \frac{2\times P\times R}{P + R} = \frac{2 \times TP}{样例总数+TP-TN}
	    $$
	
	  * 根据对查准率和查全率重视程度不同——$F_\beta$:
	      $$
	        F_\beta = \frac{(1+\beta^2)\times P\times R}{(\beta^2 \times P)+R}
	      $$
	
	        $\beta$> 1 时查全率有更大影响 ; $\beta$ < 1 时查准率有更大影响.
	
	  * 宏- & 微-
	
	     宏-：在各混淆矩阵上分别计算出查准率和查全率，再计算平均值
	
	     微-：先将各泪淆矩阵的对应元素进行平均，得到TP、FP、TN、FN的平均值，再基于平均值计算PRF1
	
	  * ……
	
* 比较检验——统计假设检验


# 3. 代价函数和梯度下降	
## 线性回归

* **回归问题**
* 1. 假设函数 - 针对x
* 2. 代价函数 - 针对theta（cost function）- **平方误差函数**（squared error function）*objective function 目标函数，就是我们要求的函数呗*
* 1参&2参（等高线图）
* optimization objective：最小化cost function，此时fits the data well

## 梯度下降
* 注意：对theta1和theta2**同步更新**（不同的求导 。同时算，同时更新 || 不是算一个更新一个学习率
* converge diverge 收敛 发散
* 已经在局部最优处 - theta1不再改变；随着越接近最优值，倒数变小，移动幅度会越来越小


## 线性代数
* 矩阵matrix和向量vector（列） - 上标表示
* 加法和标量乘法；
* **矩阵向量乘法** - 🌟可以把一些计算写成矩阵相乘的形式，简化代码，计算效率更高； 
	矩阵乘法 - 多个假设时。 —— 将大量运算打包，用一次矩阵乘法计算 
*  矩阵乘法不符合交换律，矩阵乘法也符合结合律 
*  不可逆矩阵：奇异矩阵  

## 多元线性回归
* m：样本数；n：特征数；x - 上标：第几个样本；下标：第几个特征
* 特征缩放 feature scaling：确保特征取值x都在一个相近的范围，更容易收敛。【range -1:1 数字并不严格，0：3， -2：0.5， -1/3:1/3都行，但是别太大或太小】

		如果差很多的话，可能会很慢，而且来回震荡
		
	**mean normalization 均值归一化** ➡️ -均值，/(最大值-最小值)（or 标准差）
* 判断收敛 - 看epoch-J图 。 每隔3倍设一个å[0.01, 0.03, 0.1...]，画这个图  
* 特征选择 - 🉑️特征组合 
* 模型选择 - 多项式回归（凭借对函数图像，数据形状的了解，进行选择）
* 正规方程：Θ = (X^T X)'X^T y —— 不可逆：有多余特征（线性相关）；m < n - 删除一些特征，或regularization 正规化
* 特征多，用梯度下降（10000）特征少，用正规方程

# 4. Logistic regression

logistic回归是一种分类算法。假设$h_\theta(x)$:
$$
h_\theta(x) = g(\theta^TX)\\
g = \frac{1}{1+e^{-z}}
$$
有$0\le h_\theta(x)\le1$，**是p在x, $\theta$的条件下，p=1的概率。**



## **决策边界**($\theta$决定，而不是训练集)：

predict $y = 1$, $h_\theta(x)\ge0.5$

predict $y = 0$, $h_\theta(x)\lt0.5$

那么，决策边界为：$h_\theta(x)=0.5$$\rightarrow$$z=0$



## **代价函数**

**理解**：
$$
J(\theta) = \dfrac{1}{m}\sum_{i=1}^{m}\dfrac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2= \dfrac{1}{m}\sum_{i=1}^{m}cost(h_\theta(x)-y)
$$
是假设与label的误差。



**计算**：*（极大似然法）***交叉熵函数**
$$
cost(h_\theta(x), y)=\left\{
\begin{aligned}
&-\log(h_\theta(x))		&if\ y = 1 \\
&-\log(1-h_\theta(x))	&if\ y = 0\\
\end{aligned}
=-y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x)))
\right.
$$

## 梯度下降 - one of the optimization algorithms

为了minimize $J(\theta) \rarr \theta_j := \theta_j - \alpha \colorbox{yellow} {$\frac{\partial}{\partial\theta_j}J(\theta)$}$，把$\theta$往梯度（偏导）方向（梯度下降的方向）走$\alpha$，其中：
$$
\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$
（和linear的形式一样）

## 多元分类 multi-classification

**one-vs-rest**：k类，训练k个分类器，每个分类器分一个正类，其他都是负类：
$$
h_\theta^{(i)}(x) = P(y = i|x;\theta)\quad (i = 1, 2, 3...)\\
\Darr \\ 
\underset{i}{max}\ h_\theta^{(i)}(x)
$$

# 5. regularization 正则化

## 代价函数+惩罚项（所有$\theta_j\ except\ \theta_0$）

$$
J(\theta) = \dfrac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_\colorbox{yellow}{$i=1$}^n\theta_j^2]
$$

## 梯度下降

$$
\begin{align}
\theta_0 &:= \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \qquad \theta_0不加正则化\\

\theta_j &:= \theta_j - \alpha[\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]\\

&:= \theta_j(1-\alpha\frac{\lambda}{m})- \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})y^{(i)})x_j^{(i)}
\end{align}
$$

> 对x, y：上标表示第几个样本（m），下标表示第几个特征（n）；
>
> 对神经网络（激活层a，全连接层的$\theta$）：上标表示层数，下标表示该层第几个神经元

# 6. neural network 神经网络

## logistic unit

<img src="191005 机器学习笔记总结.assets/image-20191006112015562.png" alt="image-20191006112015562" style="zoom: 33%;" />

$x$为input，通过$\theta$矩阵乘法，到中间的unit，值为$z$，然后通过$h = \dfrac{1}{1+e^{-\theta^Tx}}$
$$
x=\begin{bmatrix}
   x_0  \\
   x_1  \\
   x_2	\\
   x_3
\end{bmatrix}
\qquad
\theta=\begin{bmatrix}
   \theta_0  \\
   \theta_1  \\
   \theta_2	\\
   \theta_3
\end{bmatrix}
$$

​		$\text{size of }\theta_j:\ \colorbox{yellow}{$s_{j+1}\times (s_j+1)$} 转置\Rarr(input\ feature\ size +1)\times (output feature\ size)$	output是每次下层特征数，然后算的时候，把$\theta_0=1$加上所以特征加一

## 代价函数

$$
J(\theta) = -\dfrac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^K-y_k^{(i)}\log(h_\theta(x^{(i)}))_k +(1-y_k^{(i)})\log(1-h_\theta(x^{(i)}))_k)]+\dfrac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{S_l}\sum_{j=1}^{S_{l+1}}\theta_{ji}^{(l)}
$$

$K:$种类数；$L:$层数；$S_l:l$层的神经元个数。

## 前向传播 forward propagation & 后向传播 backpropogation

<img src="Picture/image-20191006120212663.png" alt="image-20191006120212663" style="zoom:33%;" />

**forward：**
$$
\begin{align}

a^{(1)} &= x 										&&\Rarr	&\\
z^{(2)} &= \theta^{(1)}a^{(1)}	&&\Rarr	&a^{(2)} = g(z^{(2)}) 	\\
z^{(3)} &= \theta^{(2)}a^{(2)}	&&\Rarr	&a^{(3)} = g(z^{(3)}) 	\\
z^{(4)} &= \theta^{(3)}a^{(3)}	&&\Rarr	&a^{(4)} = g(z^{(4)})

\end{align}
$$
a：激活层output，z：fc层output

**backward：**<u>求梯度</u>

对$(x^{(i)}, y^{(i)})$，忽略正则化，优化目标和代价函数：
$$
J(\theta) = \dfrac{1}{m}\sum_{i=1}^{m}cost^{(i)}\\
cost^{(i)}=-y^{(i)}\log(h_\theta(x^{(i)})) - (1-y^{(i)})\log(1-h_\theta(x^{(i)})))\approx(h_\theta(x^{(i)})-y^{(i)})^2
$$
表示神经网络预测的准确程度，$predict$与$y^i$的接近程度。

$\delta_{j}^{(l)}$是第l层，第j个单元激活项的误差：
$$
\begin{align}
\delta_j^{(4)} &= a_j^{(4)}-y_j\\
\delta_j^{(3)} &= (\theta^{(3)})^T \colorbox{yellow}{$\delta^{(4)}$}.* \colorbox{yellow}{$g'(z^{(3)})$}\quad & g'(z^{(3)}) = a^{(3)}.*(1-a^{(3)})\\
\delta_j^{(3)} &= (\theta^{(3)})^T{\delta^{(4)}}.*{g'(z^{(3)})} & g'(z^{(2)}) = a^{(2)}.*(1-a^{(2)})
\end{align}
$$

* 无$\delta^{(1)}_j$，第一层是输入层，不需要更新；

* $\delta$和$g'$是上层传来的误差，和本层的误差;
* $\delta_{j}^{(l)}$用来算偏导（$\theta_{ij}$是第l层第i个unit和第l+1层第j个unit相连的权重）：

$$
\dfrac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta) = a_j^{(l)}\delta_i^{(l+1)}
$$

​	**解释：**

【链式法则】$\dfrac{\partial J}{\partial \theta^{(3)}} = \dfrac{\partial J}{\partial a^{(4)}}\dfrac{\partial a^{(4)}}{\partial z^{(4)}}\dfrac{\partial z^{(4)}}{\partial \theta^{(3)}}$	以此类推（上层误差*本层偏导）

【计算图】**乘法门**，$\theta$的偏导=x（也就是本层的a）

然后对<u>m个样本</u>，每个样本求出来的偏导累加得到$\Delta_{ij}^{(l)}$，梯度项：
$$
\begin{align}
D_{ij}^{(l)}&:=\dfrac{1}{m}\Delta_{ij}^{(l)} && if\ j=0\\
D_{ij}^{(l)}&:=\dfrac{1}{m}[\Delta_{ij}^{(l)}+\lambda\theta_{ij}^{(l)}] && if\ j\ne0

\end{align}
$$
然后梯度下降更新权重
$$
\theta_{ij}^{(l)}:=\theta_{ij}^{(l)}-\alpha D_{ij}^{(l)}
$$

## summary：**训练nn**

1. 初始化权重
2. forward propagation：对每个$x^{(i)}$求得$h_{\theta}(x^{(i)})$
3. 算$J(\theta)$
4. backpropogation：对每个$\theta_{ij}^{(l)}$求$D_{ij}^{(l)}$

5. 梯度检测（把bp求得的梯度和数值计算的偏导比较）
6. 用油画算法和偏导来更新$\theta$使得$\min J(\theta)$

# 7. SVM

擅长学习复杂非线性方程

## 假设、优化目标与代价函数

输出：$h_{\theta}(x)=\begin{cases}1\qquad if\quad\theta^{T}x\ge0\\0 \qquad otherwise\end{cases} $

优化目标：$\underset{\theta}{\min}C\sum_{i=1}^{m}[y^{(i)}cost_{1}(\theta^{T}x^{(i)})+(1-y^{(i)})cost_{0}(\theta^{T}x^{(i)})]+\dfrac{1}{2}\sum_{i=1}^{n}\theta_{j}^{2}$

其中，代价函数：

<img src="Picture/IMG_9119(20191006-213822).jpg" alt="IMG_9119(Picture/IMG_9119(20191006-213822).jpg)" style="zoom:20%;" />

若为正类，$\theta^{T}x\ge0$即可分类正确，而$\theta^{T}x\ge1$代价函数才为0。

这里，$1, -1$就是margin factor

## Large margin classifier

优化目标中，如果C很大，那么要求$cost\to 0$，优化目标就成了：
$$
\begin{align}
\min & \dfrac{1}{2}\sum_{i=1}^n\theta_j^2\\
s.t.\quad & \theta^{T}x\ge1 && if\ y^{(i)}=1\\
					& \theta^{T}x\le-1&& if\ y^{(i)}=0\\

\end{align}
$$
点乘的几何意义：$\|\theta\|\times (x$在$\theta$上的投影$)$（$\theta$垂直于$\theta^{T}x=0$直线方向；投影有方向，有正负）

因此，要$\|\theta\|$小，而受制于$\|\theta\|\times (x$在$\theta$上的投影$)$大，则要$x$在$\theta$上的投影大，即离直线距离较大。

$\therefore SVM$会选择到训练样本的最小距离较大的假设。

这是在C很大的情况，如果C不那么大，则能更好的处理outliers

## 核函数

但是有的分类问题是线性不可分的，要如何构造新特征呢？

**landmark&similarity**

定义landmard：l，算x与l的相似度：
$$
f = similarity(x, l^{(i)})=exp(-\dfrac{\|x-l\|^2}{2\sigma^2})
$$
每个l可以定义一个f，f就是核函数（这里为一个高斯核函数），f(x)就是一个新特征。

将每个x都定义为l：$l^{(i)}=x^{(i)}$

此时对给定$x^{(i)}$，特征向量$f^{(i)} = \begin{bmatrix} f_0^{(i)}\\f_1^{(i)}\\.\\.\\.\\f_m^{(i)}\end{bmatrix}$其中$f_i^{(i)}=$1

**假设**$\to$	$if\ \theta^Tf\ge0\quad predict\ y=1$

**优化目标**$\to$	$\underset{\theta}{\min}C\sum_{i=1}^{m}[y^{(i)}cost_{1}(\theta^{T}f^{(i)})+(1-y^{(i)})cost_{0}(\theta^{T}f^{(i)})]+\dfrac{1}{2}\sum_{i=1}^{m}\theta_{j}^{2}$

### (1)  线性核

$predict\ y=1 \quad if\ \theta^Tx\ge0$

适用于：n大，m小

### (2)  高斯核

$predict\ y=1 \quad if\ \theta^Tf\ge0$

其中，$f^{(i)} = exp(-\dfrac{\|x-l^{(i)}\|^2}{2\sigma^2}),\ where\ l^{(i)}=x^{(i)}$

适用于：n小，m大

**Note:** before f, need feature scaling

# Unsupervised learning 无监督学习

## 聚类 clustering

* **K-means algorithm**

  随机初始化K类中心（cluster centroid）$\mu_1,\mu_2,...,\mu_k$

$$
\begin{align}
& Repeat:\\
& \qquad (1)\text{ cluster assignment: }c^{(i)}=\underset{K}{min}\|x^{(i)}-\mu_k\|,\ i\in[1\ to\ m]\\
& \qquad (2)\text{ move centroid: }\mu_k = ave(x^{(i)}|c^{(i)}=k),\ k\in[1\ to\ K],\text{ move centroid to } \mu_k
\end{align}
$$

* **optimization objective: **

$$
\begin{align}
& \text{cost function: }J(c^{(1)}, c^{(2)}, ..., c^{(m)},\mu_1, \mu_2, ..., \mu_k) = \dfrac{1}{m}\sum_{i=1}^{m}\|x^{(i)}-\mu_{c^{(i)}}\|^2 \\
&\underset{c^{(1)}, ..., c^{(m)},\\ \mu_1, ..., \mu_k}{\min} J(c^{(1)}, c^{(2)}, ..., c^{(m)},\mu_1, \mu_2, ..., \mu_k) \\
& \text {to be more specific:}\\
& \qquad (1)\text{ cluster assignment: }\min J, w.r.t\ c^{(1)}, ..., c^{(m)}\\
& \qquad (2)\text{ move centroid: }\min J, w.r.t\ \mu_1, \mu_2, ..., \mu_k
\end{align}
$$

* 随机初始化（适用于K=2-10的情况，种类较多不受这个影响）

  随机赋值K个样本给$\mu$

  $\because$存在局部最优的情况

  $\therefore$多次（50-1000）随机初始化，运行k-means，计算J，选择J最小时候的初始化$\mu$值

  

* 选择K
  1. Elbow method(少)
  2. 根据下游目的来选择K

## 降维

* 目的：

  1. 数据压缩（因为存在数据冗余的情况，降维可以加快算法）
  2. 可视化（to 2D or 3D，得到的特征通常不是具有物理意义的特征）

* PCA

  0. preprocessing：$x_j^{(i)}=\dfrac{x_j^{(i)}-\mu_j}{s_j}$

  1. 计算协方差矩阵：$\Sigma = \dfrac{1}{m}X^TX$，是一个$n\times n$的矩阵

  2. 计算特征向量：

     > ==奇异值分解==（svd）得到[U, S, V]，**U**$\in \R^{n\times n}$中的列向量称为左奇异向量（left-singular vectors），**V**中的列向量称为右奇异向量（right-singular vectors），**S**对角线上的值称为奇异值（singular values）。$XX^T$的特征向量是**U**的列向量，$X^TX$的特征向量是**V**的列向量，$XX^T$和$X^TX$拥有n相同的特征值，为奇异值（**S**主对角线）的平方。

     在**U**中选k个列向量$U_{reduce}=[u^{(1)}, ..., u^{(k)}]\in\R^{n\times k}$

  3. $Z=\begin{bmatrix}[u^{(1)}\\ \vdots\\ u^{(k)}]\end{bmatrix}X\\k\times 1\quad k\times n \quad n \times 1$这样就把x（n维）映射到了k维上。



   * reconstruction

     $X_{approx} = U_{reduce}Z\\n\times 1\quad n\times k\quad k\times 1$

* 如何选择k

  1. $\dfrac{\dfrac{1}{m}\sum_{i=1}^m\|x^{(i)}-x_{approx}^{(i)}\|^2}{\dfrac{1}{m}\sum_{i=1}^m\|x^{(i)}\|^2}\le0.01/0.05/0.1\cdots$是projection error/variation，说明$99\%/95\%/90\%\cdots$的方差被保留。
  2. $\dfrac{\sum_{i=1}^ks_{ii}}{\sum_{i=1}^n s_{ii}} \ge 0.99/\cdots,\text{from k=1, increse k,}$最小满足条件的那个k值，就是要选择的k

* 应用：

  * 数据压缩：

    (1) 监督学习加速

    (2) 减少存储数据所需的存储器或硬盘

  * 可视化

==**Attention**==：<u>在设计算法时，首先考虑用raw data</u>，在确实存在问题时，再考虑用PCA

