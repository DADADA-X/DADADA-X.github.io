---
layout:     post
title:      "æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“"
date:       2020-06-26 10:00:00
author:     "DadaX"
tags:
    - ä¼ ç»Ÿæœºå™¨å­¦ä¹ 
---

# 0. å­¦ä¹ èµ„æ–™

* [å´æ©è¾¾æœºå™¨å­¦ä¹ -courseraï¼šlecture&test](https://www.coursera.org/learn/machine-learning/home/welcome)
* [å´æ©è¾¾æœºå™¨å­¦ä¹ -ç½‘æ˜“äº‘è¯¾å ‚ï¼švideo](https://study.163.com/course/courseMain.htm?courseId=1004570029)
* [å´æ©è¾¾æœºå™¨å­¦ä¹ exercise-github](https://github.com/nsoojin/coursera-ml-py)
* è¥¿ç“œä¹¦

# 1. æœºå™¨å­¦ä¹ ç®€ä»‹

## ç›¸å…³æ¦‚å¿µ

æœºå™¨å­¦ä¹ çš„å®šä¹‰å¦‚ä¸‹ï¼š
>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.<br>
>ä¸€ä¸ªï¼ˆæœºå™¨å­¦ä¹ ï¼‰çš„ç¨‹åºå°±æ˜¯å¯ä»¥ä»ç»éªŒæ•°æ®Eä¸­å¯¹ä»»åŠ¡Tè¿›è¡Œå­¦ä¹ çš„ç®—æ³•ï¼Œå®ƒåœ¨ä»»åŠ¡Tä¸Šçš„æ€§èƒ½åº¦é‡Pä¼šéšç€å¯¹äºç»éªŒæ•°æ®Eçš„å­¦ä¹ è€Œå˜å¾—æ›´å¥½

ç”±äºæœºå™¨å­¦ä¹ å¿…ç„¶åˆ©ç”¨äº†æŸäº›ç»éªŒï¼Œå®ƒä»¬å¸¸å¸¸**æ•°æ®**çš„å½¢å¼å­˜åœ¨ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º**æ•°æ®é›†ï¼ˆdatasetï¼‰**ï¼Œå…¶ä¸­æ¯æ¡è®°å½•ç§°ä¸ºä¸€ä¸ª**ç¤ºä¾‹ï¼ˆinstanceï¼‰**ã€‚ä¾‹å¦‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªäººçš„æ€§åˆ«ã€å¹´é¾„å’Œèº«é«˜é¢„æµ‹ä»–æ˜¯å¦æ‚£æŸç§å¸¸è§ç–¾ç—…ï¼Œæœ‰ä»¥ä¸‹æ•°æ®ï¼š

>ï¼ˆæ€§åˆ«ï¼šç”·ï¼›å¹´é¾„ï¼š18ï¼›èº«é«˜ï¼š174ï¼›æ˜¯å¦å¾—ç—…ï¼šå¦ï¼‰
>ï¼ˆæ€§åˆ«ï¼šå¥³ï¼›å¹´é¾„ï¼š17ï¼›èº«é«˜ï¼š164ï¼›æ˜¯å¦å¾—ç—…ï¼šæ˜¯ï¼‰
>ï¼ˆæ€§åˆ«ï¼šç”·ï¼›å¹´é¾„ï¼š20ï¼›èº«é«˜ï¼š181ï¼›æ˜¯å¦å¾—ç—…ï¼šæ˜¯ï¼‰
>ï¼ˆæ€§åˆ«ï¼šå¥³ï¼›å¹´é¾„ï¼š16ï¼›èº«é«˜ï¼š161ï¼›æ˜¯å¦å¾—ç—…ï¼šæ˜¯ï¼‰ â€¦â€¦

è¿™ç»„è®°å½•çš„é›†åˆç§°ä¸ºä¸€ä¸ªâ€œæ•°æ®é›†â€ï¼Œæ¯æ¡è®°å½•ç§°ä¸ºä¸€ä¸ªæ ·æœ¬ã€‚åœ¨è®°å½•ä¸­ï¼Œå…³äºè¯¥å¯¹è±¡çš„æè¿°å‹æ•°æ®ç§°ä¸º**å±æ€§**ï¼Œç”±äºå±æ€§å¾€å¾€æœ‰å¾ˆå¤šä¸ªâ€”â€”å¦‚ä¸Šæ–‡çš„å¹´é¾„ï¼Œèº«é«˜ç­‰ï¼Œå¯ä»¥æ„æˆ**å±æ€§å‘é‡**ï¼Œè¿™äº›å‘é‡å¼ æˆçš„ç©ºé—´ç§°ä¸º**å±æ€§ç©ºé—´**æˆ–**è¾“å…¥ç©ºé—´**ã€‚è€Œæˆ‘ä»¬çš„ç®—æ³•éœ€è¦é¢„æµ‹é‚£ä¸ªé‡ï¼ˆå¦‚æœ¬ä¾‹ä¸­â€œæ˜¯å¦å¾—ç—…â€ï¼‰ç§°ä¸º**æ ‡è®°**ï¼Œåœ¨æœ‰çš„æ•°æ®é›†ä¸­å­˜åœ¨æ ‡è®°ï¼Œæœ‰çš„ä¸å­˜åœ¨ã€‚æ‰€æœ‰æ ‡è®°çš„é›†åˆç§°ä¸º**æ ‡è®°ç©ºé—´**æˆ–**è¾“å‡ºç©ºé—´**ã€‚

æˆ‘ä»¬ä»æ•°æ®ä¸­å­¦å¾—æ¨¡å‹ï¼Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¸º**è®­ç»ƒ**ï¼Œæˆ‘ä»¬ä½¿ç”¨**è®­ç»ƒæ•°æ®**ï¼Œå­¦å¾—**æ¨¡å‹**ï¼Œå¹¶è¦æ±‚è¯¥æ¨¡å‹é€‚ç”¨äºæ–°æ ·æœ¬ï¼Œç§°ä¸º**æ³›åŒ–èƒ½åŠ›**ã€‚



æˆ‘ä»¬è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹ç§°ä¸ºä¸€ä¸ª**å‡è®¾**ï¼Œæ‰€æœ‰çš„å‡è®¾ä¸€èµ·æ„æˆäº†**å‡è®¾ç©ºé—´**

å¯¹äºè®­ç»ƒçš„å¾—åˆ°å¤šä¸ªä¸åŒæ¨¡å‹ï¼Œæˆ‘ä»¬å¦‚ä½•é€‰æ‹©å‘¢ï¼Ÿå¸¸ç”¨çš„æ–¹æ³•æ˜¯å¥¥å¡å§†å‰ƒåˆ€ï¼ˆOccam's razorï¼‰ï¼š

>å¥¥å¡å§†å‰ƒåˆ€ï¼šè‹¥æœ‰å¤šä¸ªå‡è®¾ä¸è§‚å¯Ÿä¸€è‡´ï¼Œåˆ™é€‰æœ€ç®€å•çš„é‚£

## ç®—æ³•åˆ†ç±»

**æ ¹æ®è®­ç»ƒæ•°æ®æœ‰æ— æ ‡è®°ï¼ˆlabelï¼‰ï¼Œæˆ‘ä»¬å°†æœºå™¨å­¦ä¹ åˆ†ä¸ºï¼š**

* ç›‘ç£å­¦ä¹  supervised learning
* æ— ç›‘ç£å­¦ä¹  unsupervised learning
* å¼ºåŒ–å­¦ä¹  reinforcement learning â€¦â€¦

ä¸‹é¢æˆ‘ä»¬ä¾æ¬¡å¯¹ä»–ä»¬è¿›è¡Œè§£é‡Šã€‚

1. ç›‘ç£å­¦ä¹ ï¼š
2. æ— ç›‘ç£å­¦ä¹ ï¼š
3. å¼ºåŒ–å­¦ä¹ ï¼š

### ç›‘ç£å­¦ä¹ 
**æ ¹æ®è¾“å‡ºå€¼æ˜¯å¦è¿ç»­ï¼Œæˆ‘ä»¬å°†ç›‘ç£å­¦ä¹ ä»»åŠ¡åˆ†ä¸ºï¼š**

**1. å›å½’ regression**ï¼š è¿ç»­å€¼è¾“å‡ºã€‚eg. é¢ç§¯ - æˆ¿ä»·é¢„æµ‹

<img src="https://i.loli.net/2020/06/27/oUPs54wZ9JQFegN.png" alt="regression.png" style="zoom:30%;" />

**2. åˆ†ç±» classification**ï¼š ç¦»æ•£å€¼è¾“å‡ºã€‚eg. è‚¿ç˜¤å¤§å° - è‰¯æ€§/æ¶æ€§

<img src="https://i.loli.net/2020/06/27/aNFcwxfSBTVXMoU.png" alt="classification.png" style="zoom:30%;" />

### æ— ç›‘ç£å­¦ä¹ 

**èšç±»**â€”â€”eg. é¸¡å°¾é…’ä¼šç®—æ³•

[W,s,v] = svd((repmat(sum(x.\*x,1),size(x,1),1).\*x)\*x');

## æ›´å¤š
* æ¨èç”¨**octave**æ¥å®ç°ç®—æ³•åŸå‹ï¼Œå†ç”¨å…¶ä»–çš„ç¼–ç¨‹è¯­è¨€æ¥å¼€å‘

# 2. æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©

## ç»éªŒè¯¯å·®ä¸è¿‡æ‹Ÿåˆ

* **é”™è¯¯ç‡ Eï¼ˆerror rateï¼‰ï¼š** *åˆ†ç±»é”™è¯¯*çš„æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹
* **ç²¾åº¦ï¼ˆaccuracyï¼‰ï¼š**ï¼ˆ1 - é”™è¯¯ç‡ï¼‰%
* **è¯¯å·®ï¼ˆerrorï¼‰ï¼š**é¢„æµ‹è¾“å‡ºä¸æ ·æœ¬çœŸæ˜¯å€¼çš„å·®å¼‚
* **è¿‡æ‹ŸåˆğŸ™** & **æ¬ æ‹Ÿåˆ** â€”â€” è¦æ±‚å­¦ä¹ é€‚ç”¨äºæ‰€æœ‰æ½œåœ¨æ ·æœ¬çš„â€œæ™®éè§„å¾‹â€

## å¦‚ä½•è¿›è¡Œæ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©â€”â€”è¯„ä¼°æ–¹æ³•
>ä»**æ•°æ®é›†D**ä¸­äº§ç”Ÿ**è®­ç»ƒé›†S**å’Œ**æµ‹è¯•é›†T**

* ç•™å‡ºæ³•ï¼ˆhold-outï¼‰ï¼šç›´æ¥å°†æ•°æ®é›†Dåˆ’åˆ†ä¸ºä¸¤ä¸ªäº’æ–¥çš„é›†åˆï¼ˆ*2/3ï½4/5*çš„æ ·æœ¬ç”¨äºè®­ç»ƒï¼‰

	âš ï¸ä¿æŒ**æ•°æ®åˆ†å¸ƒ**çš„ä¸€è‡´æ€§

		ç‹¬ç«‹åŒåˆ†å¸ƒ i.i.dï¼šæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯ç‹¬ç«‹åœ°ä»è¿™ä¸ªåˆ†å¸ƒä¸Šé‡‡æ ·è·å¾—
	
	âš ï¸å•æ¬¡ç•™å‡ºæ³•å¾—åˆ°çš„ä¼°è®¡ç»“æœå¾€å¾€ä¸å¤Ÿç¨³å®šå¯é ï¼Œä¸€èˆ¬é‡‡ç”¨è‹¥å¹²æ¬¡éšæœºåˆ’åˆ†ã€é‡å¤å®éªŒåå–å¹³å‡å€¼
	
* äº¤å‰éªŒè¯æ³•ï¼ˆk-fold cross validation(10)ï¼‰

<img src="https://i.loli.net/2020/06/27/4vGgxdPjoApOMrw.png" alt="äº¤å‰éªŒè¯.png" style="zoom: 50%;" />

* è‡ªåŠ©æ³•ï¼ˆå°‘ï¼‰

 ä»Dä¸­æœ‰æ”¾å›çš„éšæœºæŒ‘é€‰mä¸ªæ ·æœ¬æ”¾å…¥**D'**åšè®­ç»ƒé›†ï¼Œ**D\D'**åšæµ‹è¯•é›†

 ğŸ’­ç”¨äºæ•°æ®é›†å°ã€éš¾äºæœ‰æ•ˆåˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†æ—¶

>è°ƒå‚ä¸æœ€ç»ˆæ¨¡å‹

* æµ‹è¯•é›†ï¼šä¼°è®¡æ¨¡å‹åœ¨å®é™…ä½¿ç”¨æ—¶çš„æ³›åŒ–èƒ½åŠ›
* éªŒè¯é›†ï¼šæ¨¡å‹é€‰æ‹©å’Œè°ƒå‚

## æ€§èƒ½åº¦é‡ performance measure
* å›å½’ä»»åŠ¡æœ€å¸¸ç”¨çš„æ€§èƒ½åº¦é‡ï¼š**å‡æ–¹è¯¯å·®ï¼ˆmean squared errorï¼‰**

  
  $$
  E(f;D)=\frac{1}{m}\sum^m_{i=1}(f(x_i)-y_i)^2
  $$

* åˆ†ç±»é—®é¢˜å¸¸ç”¨æ€§èƒ½åº¦é‡ï¼š

	* é”™è¯¯ç‡ä¸ç²¾åº¦
	  $$
  E(f;D)=\frac{1}{m}\sum^m_{i=1}\mathbb I(f(x_i)\ne y_i)
	  $$
	  
	* æŸ¥å‡†ç‡ï¼ˆprecisionï¼‰ã€æŸ¥å…¨ç‡(recall)ä¸F1
	
	<img src="https://i.loli.net/2020/06/27/Zt7aQn1TsMcYXhg.png" alt="P&amp;R.png" style="zoom:33%;" />
	
	* é‚£ä¹ˆå¦‚ä½•æ ¹æ®På’ŒRåˆ¤æ–­å­¦ä¹ æœŸçš„æ€§èƒ½å‘¢ï¼Ÿ
	
	  ![PRæ›²çº¿.png](https://i.loli.net/2020/06/27/GVCrBjNkAMPaQ15.png)
	
	     * æ ¹æ®æ›²çº¿ï¼šCæ›²çº¿è¢«Aå…¨åŒ…ä½ï¼ŒAä¼˜äºC
	     * æ›²çº¿ä¸‹é¢ç§¯å¤§å°ï¼šé¢ç§¯å¤§çš„å¥½
	
	  
	
	  â¬†ï¸ä¸æ˜“ä¼°ç®—
	
	  
	
	  * å¹³è¡¡ç‚¹ï¼ˆbreak even point, BEPï¼‰:P = R, å¤§çš„å¥½ï¼ŒAä¼˜äºB
	    	
	
	  * **F1**
	    $$
	        F1 = \frac{2\times P\times R}{P + R} = \frac{2 \times TP}{æ ·ä¾‹æ€»æ•°+TP-TN}
	    $$
	
	  * æ ¹æ®å¯¹æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡é‡è§†ç¨‹åº¦ä¸åŒâ€”â€”$F_\beta$:
	      $$
	        F_\beta = \frac{(1+\beta^2)\times P\times R}{(\beta^2 \times P)+R}
	      $$
	
	        $\beta$> 1 æ—¶æŸ¥å…¨ç‡æœ‰æ›´å¤§å½±å“ ; $\beta$ < 1 æ—¶æŸ¥å‡†ç‡æœ‰æ›´å¤§å½±å“.
	
	  * å®- & å¾®-
	
	     å®-ï¼šåœ¨å„æ··æ·†çŸ©é˜µä¸Šåˆ†åˆ«è®¡ç®—å‡ºæŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡ï¼Œå†è®¡ç®—å¹³å‡å€¼
	
	     å¾®-ï¼šå…ˆå°†å„æ³ªæ·†çŸ©é˜µçš„å¯¹åº”å…ƒç´ è¿›è¡Œå¹³å‡ï¼Œå¾—åˆ°TPã€FPã€TNã€FNçš„å¹³å‡å€¼ï¼Œå†åŸºäºå¹³å‡å€¼è®¡ç®—PRF1
	
	  * â€¦â€¦
	
* æ¯”è¾ƒæ£€éªŒâ€”â€”ç»Ÿè®¡å‡è®¾æ£€éªŒ


# 3. ä»£ä»·å‡½æ•°å’Œæ¢¯åº¦ä¸‹é™	
## çº¿æ€§å›å½’

* **å›å½’é—®é¢˜**
* 1. å‡è®¾å‡½æ•° - é’ˆå¯¹x
* 2. ä»£ä»·å‡½æ•° - é’ˆå¯¹thetaï¼ˆcost functionï¼‰- **å¹³æ–¹è¯¯å·®å‡½æ•°**ï¼ˆsquared error functionï¼‰*objective function ç›®æ ‡å‡½æ•°ï¼Œå°±æ˜¯æˆ‘ä»¬è¦æ±‚çš„å‡½æ•°å‘—*
* 1å‚&2å‚ï¼ˆç­‰é«˜çº¿å›¾ï¼‰
* optimization objectiveï¼šæœ€å°åŒ–cost functionï¼Œæ­¤æ—¶fits the data well

## æ¢¯åº¦ä¸‹é™
* æ³¨æ„ï¼šå¯¹theta1å’Œtheta2**åŒæ­¥æ›´æ–°**ï¼ˆä¸åŒçš„æ±‚å¯¼ ã€‚åŒæ—¶ç®—ï¼ŒåŒæ—¶æ›´æ–° || ä¸æ˜¯ç®—ä¸€ä¸ªæ›´æ–°ä¸€ä¸ªå­¦ä¹ ç‡
* converge diverge æ”¶æ•› å‘æ•£
* å·²ç»åœ¨å±€éƒ¨æœ€ä¼˜å¤„ - theta1ä¸å†æ”¹å˜ï¼›éšç€è¶Šæ¥è¿‘æœ€ä¼˜å€¼ï¼Œå€’æ•°å˜å°ï¼Œç§»åŠ¨å¹…åº¦ä¼šè¶Šæ¥è¶Šå°


## çº¿æ€§ä»£æ•°
* çŸ©é˜µmatrixå’Œå‘é‡vectorï¼ˆåˆ—ï¼‰ - ä¸Šæ ‡è¡¨ç¤º
* åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•ï¼›
* **çŸ©é˜µå‘é‡ä¹˜æ³•** - ğŸŒŸå¯ä»¥æŠŠä¸€äº›è®¡ç®—å†™æˆçŸ©é˜µç›¸ä¹˜çš„å½¢å¼ï¼Œç®€åŒ–ä»£ç ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ï¼› 
	çŸ©é˜µä¹˜æ³• - å¤šä¸ªå‡è®¾æ—¶ã€‚ â€”â€” å°†å¤§é‡è¿ç®—æ‰“åŒ…ï¼Œç”¨ä¸€æ¬¡çŸ©é˜µä¹˜æ³•è®¡ç®— 
*  çŸ©é˜µä¹˜æ³•ä¸ç¬¦åˆäº¤æ¢å¾‹ï¼ŒçŸ©é˜µä¹˜æ³•ä¹Ÿç¬¦åˆç»“åˆå¾‹ 
*  ä¸å¯é€†çŸ©é˜µï¼šå¥‡å¼‚çŸ©é˜µ  

## å¤šå…ƒçº¿æ€§å›å½’
* mï¼šæ ·æœ¬æ•°ï¼›nï¼šç‰¹å¾æ•°ï¼›x - ä¸Šæ ‡ï¼šç¬¬å‡ ä¸ªæ ·æœ¬ï¼›ä¸‹æ ‡ï¼šç¬¬å‡ ä¸ªç‰¹å¾
* ç‰¹å¾ç¼©æ”¾ feature scalingï¼šç¡®ä¿ç‰¹å¾å–å€¼xéƒ½åœ¨ä¸€ä¸ªç›¸è¿‘çš„èŒƒå›´ï¼Œæ›´å®¹æ˜“æ”¶æ•›ã€‚ã€range -1:1 æ•°å­—å¹¶ä¸ä¸¥æ ¼ï¼Œ0ï¼š3ï¼Œ -2ï¼š0.5ï¼Œ -1/3:1/3éƒ½è¡Œï¼Œä½†æ˜¯åˆ«å¤ªå¤§æˆ–å¤ªå°ã€‘

		å¦‚æœå·®å¾ˆå¤šçš„è¯ï¼Œå¯èƒ½ä¼šå¾ˆæ…¢ï¼Œè€Œä¸”æ¥å›éœ‡è¡
		
	**mean normalization å‡å€¼å½’ä¸€åŒ–** â¡ï¸ -å‡å€¼ï¼Œ/(æœ€å¤§å€¼-æœ€å°å€¼)ï¼ˆor æ ‡å‡†å·®ï¼‰
* åˆ¤æ–­æ”¶æ•› - çœ‹epoch-Jå›¾ ã€‚ æ¯éš”3å€è®¾ä¸€ä¸ªÃ¥[0.01, 0.03, 0.1...]ï¼Œç”»è¿™ä¸ªå›¾  
* ç‰¹å¾é€‰æ‹© - ğŸ‰‘ï¸ç‰¹å¾ç»„åˆ 
* æ¨¡å‹é€‰æ‹© - å¤šé¡¹å¼å›å½’ï¼ˆå‡­å€Ÿå¯¹å‡½æ•°å›¾åƒï¼Œæ•°æ®å½¢çŠ¶çš„äº†è§£ï¼Œè¿›è¡Œé€‰æ‹©ï¼‰
* æ­£è§„æ–¹ç¨‹ï¼šÎ˜ = (X^T X)'X^T y â€”â€” ä¸å¯é€†ï¼šæœ‰å¤šä½™ç‰¹å¾ï¼ˆçº¿æ€§ç›¸å…³ï¼‰ï¼›m < n - åˆ é™¤ä¸€äº›ç‰¹å¾ï¼Œæˆ–regularization æ­£è§„åŒ–
* ç‰¹å¾å¤šï¼Œç”¨æ¢¯åº¦ä¸‹é™ï¼ˆ10000ï¼‰ç‰¹å¾å°‘ï¼Œç”¨æ­£è§„æ–¹ç¨‹

# 4. Logistic regression

logisticå›å½’æ˜¯ä¸€ç§åˆ†ç±»ç®—æ³•ã€‚å‡è®¾$h_\theta(x)$:
$$
h_\theta(x) = g(\theta^TX)\\
g = \frac{1}{1+e^{-z}}
$$
æœ‰$0\le h_\theta(x)\le1$ï¼Œ**æ˜¯påœ¨x, $\theta$çš„æ¡ä»¶ä¸‹ï¼Œp=1çš„æ¦‚ç‡ã€‚**



## **å†³ç­–è¾¹ç•Œ**($\theta$å†³å®šï¼Œè€Œä¸æ˜¯è®­ç»ƒé›†)ï¼š

predict $y = 1$, $h_\theta(x)\ge0.5$

predict $y = 0$, $h_\theta(x)\lt0.5$

é‚£ä¹ˆï¼Œå†³ç­–è¾¹ç•Œä¸ºï¼š$h_\theta(x)=0.5$$\rightarrow$$z=0$



## **ä»£ä»·å‡½æ•°**

**ç†è§£**ï¼š
$$
J(\theta) = \dfrac{1}{m}\sum_{i=1}^{m}\dfrac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2= \dfrac{1}{m}\sum_{i=1}^{m}cost(h_\theta(x)-y)
$$
æ˜¯å‡è®¾ä¸labelçš„è¯¯å·®ã€‚



**è®¡ç®—**ï¼š*ï¼ˆæå¤§ä¼¼ç„¶æ³•ï¼‰***äº¤å‰ç†µå‡½æ•°**
$$
cost(h_\theta(x), y)=\left\{
\begin{aligned}
&-\log(h_\theta(x))		&if\ y = 1 \\
&-\log(1-h_\theta(x))	&if\ y = 0\\
\end{aligned}
=-y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x)))
\right.
$$

## æ¢¯åº¦ä¸‹é™ - one of the optimization algorithms

ä¸ºäº†minimize $J(\theta) \rarr \theta_j := \theta_j - \alpha \colorbox{yellow} {$\frac{\partial}{\partial\theta_j}J(\theta)$}$ï¼ŒæŠŠ$\theta$å¾€æ¢¯åº¦ï¼ˆåå¯¼ï¼‰æ–¹å‘ï¼ˆæ¢¯åº¦ä¸‹é™çš„æ–¹å‘ï¼‰èµ°$\alpha$ï¼Œå…¶ä¸­ï¼š
$$
\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$
ï¼ˆå’Œlinearçš„å½¢å¼ä¸€æ ·ï¼‰

## å¤šå…ƒåˆ†ç±» multi-classification

**one-vs-rest**ï¼škç±»ï¼Œè®­ç»ƒkä¸ªåˆ†ç±»å™¨ï¼Œæ¯ä¸ªåˆ†ç±»å™¨åˆ†ä¸€ä¸ªæ­£ç±»ï¼Œå…¶ä»–éƒ½æ˜¯è´Ÿç±»ï¼š
$$
h_\theta^{(i)}(x) = P(y = i|x;\theta)\quad (i = 1, 2, 3...)\\
\Darr \\ 
\underset{i}{max}\ h_\theta^{(i)}(x)
$$

# 5. regularization æ­£åˆ™åŒ–

## ä»£ä»·å‡½æ•°+æƒ©ç½šé¡¹ï¼ˆæ‰€æœ‰$\theta_j\ except\ \theta_0$ï¼‰

$$
J(\theta) = \dfrac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_\colorbox{yellow}{$i=1$}^n\theta_j^2]
$$

## æ¢¯åº¦ä¸‹é™

$$
\begin{align}
\theta_0 &:= \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \qquad \theta_0ä¸åŠ æ­£åˆ™åŒ–\\

\theta_j &:= \theta_j - \alpha[\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]\\

&:= \theta_j(1-\alpha\frac{\lambda}{m})- \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})y^{(i)})x_j^{(i)}
\end{align}
$$

> å¯¹x, yï¼šä¸Šæ ‡è¡¨ç¤ºç¬¬å‡ ä¸ªæ ·æœ¬ï¼ˆmï¼‰ï¼Œä¸‹æ ‡è¡¨ç¤ºç¬¬å‡ ä¸ªç‰¹å¾ï¼ˆnï¼‰ï¼›
>
> å¯¹ç¥ç»ç½‘ç»œï¼ˆæ¿€æ´»å±‚aï¼Œå…¨è¿æ¥å±‚çš„$\theta$ï¼‰ï¼šä¸Šæ ‡è¡¨ç¤ºå±‚æ•°ï¼Œä¸‹æ ‡è¡¨ç¤ºè¯¥å±‚ç¬¬å‡ ä¸ªç¥ç»å…ƒ

# 6. neural network ç¥ç»ç½‘ç»œ

## logistic unit

<img src="191005 æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“.assets/image-20191006112015562.png" alt="image-20191006112015562" style="zoom: 33%;" />

$x$ä¸ºinputï¼Œé€šè¿‡$\theta$çŸ©é˜µä¹˜æ³•ï¼Œåˆ°ä¸­é—´çš„unitï¼Œå€¼ä¸º$z$ï¼Œç„¶åé€šè¿‡$h = \dfrac{1}{1+e^{-\theta^Tx}}$
$$
x=\begin{bmatrix}
   x_0  \\
   x_1  \\
   x_2	\\
   x_3
\end{bmatrix}
\qquad
\theta=\begin{bmatrix}
   \theta_0  \\
   \theta_1  \\
   \theta_2	\\
   \theta_3
\end{bmatrix}
$$

â€‹		$\text{size of }\theta_j:\ \colorbox{yellow}{$s_{j+1}\times (s_j+1)$} è½¬ç½®\Rarr(input\ feature\ size +1)\times (output feature\ size)$	outputæ˜¯æ¯æ¬¡ä¸‹å±‚ç‰¹å¾æ•°ï¼Œç„¶åç®—çš„æ—¶å€™ï¼ŒæŠŠ$\theta_0=1$åŠ ä¸Šæ‰€ä»¥ç‰¹å¾åŠ ä¸€

## ä»£ä»·å‡½æ•°

$$
J(\theta) = -\dfrac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^K-y_k^{(i)}\log(h_\theta(x^{(i)}))_k +(1-y_k^{(i)})\log(1-h_\theta(x^{(i)}))_k)]+\dfrac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{S_l}\sum_{j=1}^{S_{l+1}}\theta_{ji}^{(l)}
$$

$K:$ç§ç±»æ•°ï¼›$L:$å±‚æ•°ï¼›$S_l:l$å±‚çš„ç¥ç»å…ƒä¸ªæ•°ã€‚

## å‰å‘ä¼ æ’­ forward propagation & åå‘ä¼ æ’­ backpropogation

<img src="Picture/image-20191006120212663.png" alt="image-20191006120212663" style="zoom:33%;" />

**forwardï¼š**
$$
\begin{align}

a^{(1)} &= x 										&&\Rarr	&\\
z^{(2)} &= \theta^{(1)}a^{(1)}	&&\Rarr	&a^{(2)} = g(z^{(2)}) 	\\
z^{(3)} &= \theta^{(2)}a^{(2)}	&&\Rarr	&a^{(3)} = g(z^{(3)}) 	\\
z^{(4)} &= \theta^{(3)}a^{(3)}	&&\Rarr	&a^{(4)} = g(z^{(4)})

\end{align}
$$
aï¼šæ¿€æ´»å±‚outputï¼Œzï¼šfcå±‚output

**backwardï¼š**<u>æ±‚æ¢¯åº¦</u>

å¯¹$(x^{(i)}, y^{(i)})$ï¼Œå¿½ç•¥æ­£åˆ™åŒ–ï¼Œä¼˜åŒ–ç›®æ ‡å’Œä»£ä»·å‡½æ•°ï¼š
$$
J(\theta) = \dfrac{1}{m}\sum_{i=1}^{m}cost^{(i)}\\
cost^{(i)}=-y^{(i)}\log(h_\theta(x^{(i)})) - (1-y^{(i)})\log(1-h_\theta(x^{(i)})))\approx(h_\theta(x^{(i)})-y^{(i)})^2
$$
è¡¨ç¤ºç¥ç»ç½‘ç»œé¢„æµ‹çš„å‡†ç¡®ç¨‹åº¦ï¼Œ$predict$ä¸$y^i$çš„æ¥è¿‘ç¨‹åº¦ã€‚

$\delta_{j}^{(l)}$æ˜¯ç¬¬lå±‚ï¼Œç¬¬jä¸ªå•å…ƒæ¿€æ´»é¡¹çš„è¯¯å·®ï¼š
$$
\begin{align}
\delta_j^{(4)} &= a_j^{(4)}-y_j\\
\delta_j^{(3)} &= (\theta^{(3)})^T \colorbox{yellow}{$\delta^{(4)}$}.* \colorbox{yellow}{$g'(z^{(3)})$}\quad & g'(z^{(3)}) = a^{(3)}.*(1-a^{(3)})\\
\delta_j^{(3)} &= (\theta^{(3)})^T{\delta^{(4)}}.*{g'(z^{(3)})} & g'(z^{(2)}) = a^{(2)}.*(1-a^{(2)})
\end{align}
$$

* æ— $\delta^{(1)}_j$ï¼Œç¬¬ä¸€å±‚æ˜¯è¾“å…¥å±‚ï¼Œä¸éœ€è¦æ›´æ–°ï¼›

* $\delta$å’Œ$g'$æ˜¯ä¸Šå±‚ä¼ æ¥çš„è¯¯å·®ï¼Œå’Œæœ¬å±‚çš„è¯¯å·®;
* $\delta_{j}^{(l)}$ç”¨æ¥ç®—åå¯¼ï¼ˆ$\theta_{ij}$æ˜¯ç¬¬lå±‚ç¬¬iä¸ªunitå’Œç¬¬l+1å±‚ç¬¬jä¸ªunitç›¸è¿çš„æƒé‡ï¼‰ï¼š

$$
\dfrac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta) = a_j^{(l)}\delta_i^{(l+1)}
$$

â€‹	**è§£é‡Šï¼š**

ã€é“¾å¼æ³•åˆ™ã€‘$\dfrac{\partial J}{\partial \theta^{(3)}} = \dfrac{\partial J}{\partial a^{(4)}}\dfrac{\partial a^{(4)}}{\partial z^{(4)}}\dfrac{\partial z^{(4)}}{\partial \theta^{(3)}}$	ä»¥æ­¤ç±»æ¨ï¼ˆä¸Šå±‚è¯¯å·®*æœ¬å±‚åå¯¼ï¼‰

ã€è®¡ç®—å›¾ã€‘**ä¹˜æ³•é—¨**ï¼Œ$\theta$çš„åå¯¼=xï¼ˆä¹Ÿå°±æ˜¯æœ¬å±‚çš„aï¼‰

ç„¶åå¯¹<u>mä¸ªæ ·æœ¬</u>ï¼Œæ¯ä¸ªæ ·æœ¬æ±‚å‡ºæ¥çš„åå¯¼ç´¯åŠ å¾—åˆ°$\Delta_{ij}^{(l)}$ï¼Œæ¢¯åº¦é¡¹ï¼š
$$
\begin{align}
D_{ij}^{(l)}&:=\dfrac{1}{m}\Delta_{ij}^{(l)} && if\ j=0\\
D_{ij}^{(l)}&:=\dfrac{1}{m}[\Delta_{ij}^{(l)}+\lambda\theta_{ij}^{(l)}] && if\ j\ne0

\end{align}
$$
ç„¶åæ¢¯åº¦ä¸‹é™æ›´æ–°æƒé‡
$$
\theta_{ij}^{(l)}:=\theta_{ij}^{(l)}-\alpha D_{ij}^{(l)}
$$

## summaryï¼š**è®­ç»ƒnn**

1. åˆå§‹åŒ–æƒé‡
2. forward propagationï¼šå¯¹æ¯ä¸ª$x^{(i)}$æ±‚å¾—$h_{\theta}(x^{(i)})$
3. ç®—$J(\theta)$
4. backpropogationï¼šå¯¹æ¯ä¸ª$\theta_{ij}^{(l)}$æ±‚$D_{ij}^{(l)}$

5. æ¢¯åº¦æ£€æµ‹ï¼ˆæŠŠbpæ±‚å¾—çš„æ¢¯åº¦å’Œæ•°å€¼è®¡ç®—çš„åå¯¼æ¯”è¾ƒï¼‰
6. ç”¨æ²¹ç”»ç®—æ³•å’Œåå¯¼æ¥æ›´æ–°$\theta$ä½¿å¾—$\min J(\theta)$

# 7. SVM

æ“…é•¿å­¦ä¹ å¤æ‚éçº¿æ€§æ–¹ç¨‹

## å‡è®¾ã€ä¼˜åŒ–ç›®æ ‡ä¸ä»£ä»·å‡½æ•°

è¾“å‡ºï¼š$h_{\theta}(x)=\begin{cases}1\qquad if\quad\theta^{T}x\ge0\\0 \qquad otherwise\end{cases} $

ä¼˜åŒ–ç›®æ ‡ï¼š$\underset{\theta}{\min}C\sum_{i=1}^{m}[y^{(i)}cost_{1}(\theta^{T}x^{(i)})+(1-y^{(i)})cost_{0}(\theta^{T}x^{(i)})]+\dfrac{1}{2}\sum_{i=1}^{n}\theta_{j}^{2}$

å…¶ä¸­ï¼Œä»£ä»·å‡½æ•°ï¼š

<img src="Picture/IMG_9119(20191006-213822).jpg" alt="IMG_9119(Picture/IMG_9119(20191006-213822).jpg)" style="zoom:20%;" />

è‹¥ä¸ºæ­£ç±»ï¼Œ$\theta^{T}x\ge0$å³å¯åˆ†ç±»æ­£ç¡®ï¼Œè€Œ$\theta^{T}x\ge1$ä»£ä»·å‡½æ•°æ‰ä¸º0ã€‚

è¿™é‡Œï¼Œ$1, -1$å°±æ˜¯margin factor

## Large margin classifier

ä¼˜åŒ–ç›®æ ‡ä¸­ï¼Œå¦‚æœCå¾ˆå¤§ï¼Œé‚£ä¹ˆè¦æ±‚$cost\to 0$ï¼Œä¼˜åŒ–ç›®æ ‡å°±æˆäº†ï¼š
$$
\begin{align}
\min & \dfrac{1}{2}\sum_{i=1}^n\theta_j^2\\
s.t.\quad & \theta^{T}x\ge1 && if\ y^{(i)}=1\\
					& \theta^{T}x\le-1&& if\ y^{(i)}=0\\

\end{align}
$$
ç‚¹ä¹˜çš„å‡ ä½•æ„ä¹‰ï¼š$\|\theta\|\times (x$åœ¨$\theta$ä¸Šçš„æŠ•å½±$)$ï¼ˆ$\theta$å‚ç›´äº$\theta^{T}x=0$ç›´çº¿æ–¹å‘ï¼›æŠ•å½±æœ‰æ–¹å‘ï¼Œæœ‰æ­£è´Ÿï¼‰

å› æ­¤ï¼Œè¦$\|\theta\|$å°ï¼Œè€Œå—åˆ¶äº$\|\theta\|\times (x$åœ¨$\theta$ä¸Šçš„æŠ•å½±$)$å¤§ï¼Œåˆ™è¦$x$åœ¨$\theta$ä¸Šçš„æŠ•å½±å¤§ï¼Œå³ç¦»ç›´çº¿è·ç¦»è¾ƒå¤§ã€‚

$\therefore SVM$ä¼šé€‰æ‹©åˆ°è®­ç»ƒæ ·æœ¬çš„æœ€å°è·ç¦»è¾ƒå¤§çš„å‡è®¾ã€‚

è¿™æ˜¯åœ¨Cå¾ˆå¤§çš„æƒ…å†µï¼Œå¦‚æœCä¸é‚£ä¹ˆå¤§ï¼Œåˆ™èƒ½æ›´å¥½çš„å¤„ç†outliers

## æ ¸å‡½æ•°

ä½†æ˜¯æœ‰çš„åˆ†ç±»é—®é¢˜æ˜¯çº¿æ€§ä¸å¯åˆ†çš„ï¼Œè¦å¦‚ä½•æ„é€ æ–°ç‰¹å¾å‘¢ï¼Ÿ

**landmark&similarity**

å®šä¹‰landmardï¼šlï¼Œç®—xä¸lçš„ç›¸ä¼¼åº¦ï¼š
$$
f = similarity(x, l^{(i)})=exp(-\dfrac{\|x-l\|^2}{2\sigma^2})
$$
æ¯ä¸ªlå¯ä»¥å®šä¹‰ä¸€ä¸ªfï¼Œfå°±æ˜¯æ ¸å‡½æ•°ï¼ˆè¿™é‡Œä¸ºä¸€ä¸ªé«˜æ–¯æ ¸å‡½æ•°ï¼‰ï¼Œf(x)å°±æ˜¯ä¸€ä¸ªæ–°ç‰¹å¾ã€‚

å°†æ¯ä¸ªxéƒ½å®šä¹‰ä¸ºlï¼š$l^{(i)}=x^{(i)}$

æ­¤æ—¶å¯¹ç»™å®š$x^{(i)}$ï¼Œç‰¹å¾å‘é‡$f^{(i)} = \begin{bmatrix} f_0^{(i)}\\f_1^{(i)}\\.\\.\\.\\f_m^{(i)}\end{bmatrix}$å…¶ä¸­$f_i^{(i)}=$1

**å‡è®¾**$\to$	$if\ \theta^Tf\ge0\quad predict\ y=1$

**ä¼˜åŒ–ç›®æ ‡**$\to$	$\underset{\theta}{\min}C\sum_{i=1}^{m}[y^{(i)}cost_{1}(\theta^{T}f^{(i)})+(1-y^{(i)})cost_{0}(\theta^{T}f^{(i)})]+\dfrac{1}{2}\sum_{i=1}^{m}\theta_{j}^{2}$

### (1)  çº¿æ€§æ ¸

$predict\ y=1 \quad if\ \theta^Tx\ge0$

é€‚ç”¨äºï¼šnå¤§ï¼Œmå°

### (2)  é«˜æ–¯æ ¸

$predict\ y=1 \quad if\ \theta^Tf\ge0$

å…¶ä¸­ï¼Œ$f^{(i)} = exp(-\dfrac{\|x-l^{(i)}\|^2}{2\sigma^2}),\ where\ l^{(i)}=x^{(i)}$

é€‚ç”¨äºï¼šnå°ï¼Œmå¤§

**Note:** before f, need feature scaling

# Unsupervised learning æ— ç›‘ç£å­¦ä¹ 

## èšç±» clustering

* **K-means algorithm**

  éšæœºåˆå§‹åŒ–Kç±»ä¸­å¿ƒï¼ˆcluster centroidï¼‰$\mu_1,\mu_2,...,\mu_k$

$$
\begin{align}
& Repeat:\\
& \qquad (1)\text{ cluster assignment: }c^{(i)}=\underset{K}{min}\|x^{(i)}-\mu_k\|,\ i\in[1\ to\ m]\\
& \qquad (2)\text{ move centroid: }\mu_k = ave(x^{(i)}|c^{(i)}=k),\ k\in[1\ to\ K],\text{ move centroid to } \mu_k
\end{align}
$$

* **optimization objective: **

$$
\begin{align}
& \text{cost function: }J(c^{(1)}, c^{(2)}, ..., c^{(m)},\mu_1, \mu_2, ..., \mu_k) = \dfrac{1}{m}\sum_{i=1}^{m}\|x^{(i)}-\mu_{c^{(i)}}\|^2 \\
&\underset{c^{(1)}, ..., c^{(m)},\\ \mu_1, ..., \mu_k}{\min} J(c^{(1)}, c^{(2)}, ..., c^{(m)},\mu_1, \mu_2, ..., \mu_k) \\
& \text {to be more specific:}\\
& \qquad (1)\text{ cluster assignment: }\min J, w.r.t\ c^{(1)}, ..., c^{(m)}\\
& \qquad (2)\text{ move centroid: }\min J, w.r.t\ \mu_1, \mu_2, ..., \mu_k
\end{align}
$$

* éšæœºåˆå§‹åŒ–ï¼ˆé€‚ç”¨äºK=2-10çš„æƒ…å†µï¼Œç§ç±»è¾ƒå¤šä¸å—è¿™ä¸ªå½±å“ï¼‰

  éšæœºèµ‹å€¼Kä¸ªæ ·æœ¬ç»™$\mu$

  $\because$å­˜åœ¨å±€éƒ¨æœ€ä¼˜çš„æƒ…å†µ

  $\therefore$å¤šæ¬¡ï¼ˆ50-1000ï¼‰éšæœºåˆå§‹åŒ–ï¼Œè¿è¡Œk-meansï¼Œè®¡ç®—Jï¼Œé€‰æ‹©Jæœ€å°æ—¶å€™çš„åˆå§‹åŒ–$\mu$å€¼

  

* é€‰æ‹©K
  1. Elbow method(å°‘)
  2. æ ¹æ®ä¸‹æ¸¸ç›®çš„æ¥é€‰æ‹©K

## é™ç»´

* ç›®çš„ï¼š

  1. æ•°æ®å‹ç¼©ï¼ˆå› ä¸ºå­˜åœ¨æ•°æ®å†—ä½™çš„æƒ…å†µï¼Œé™ç»´å¯ä»¥åŠ å¿«ç®—æ³•ï¼‰
  2. å¯è§†åŒ–ï¼ˆto 2D or 3Dï¼Œå¾—åˆ°çš„ç‰¹å¾é€šå¸¸ä¸æ˜¯å…·æœ‰ç‰©ç†æ„ä¹‰çš„ç‰¹å¾ï¼‰

* PCA

  0. preprocessingï¼š$x_j^{(i)}=\dfrac{x_j^{(i)}-\mu_j}{s_j}$

  1. è®¡ç®—åæ–¹å·®çŸ©é˜µï¼š$\Sigma = \dfrac{1}{m}X^TX$ï¼Œæ˜¯ä¸€ä¸ª$n\times n$çš„çŸ©é˜µ

  2. è®¡ç®—ç‰¹å¾å‘é‡ï¼š

     > ==å¥‡å¼‚å€¼åˆ†è§£==ï¼ˆsvdï¼‰å¾—åˆ°[U, S, V]ï¼Œ**U**$\in \R^{n\times n}$ä¸­çš„åˆ—å‘é‡ç§°ä¸ºå·¦å¥‡å¼‚å‘é‡ï¼ˆleft-singular vectorsï¼‰ï¼Œ**V**ä¸­çš„åˆ—å‘é‡ç§°ä¸ºå³å¥‡å¼‚å‘é‡ï¼ˆright-singular vectorsï¼‰ï¼Œ**S**å¯¹è§’çº¿ä¸Šçš„å€¼ç§°ä¸ºå¥‡å¼‚å€¼ï¼ˆsingular valuesï¼‰ã€‚$XX^T$çš„ç‰¹å¾å‘é‡æ˜¯**U**çš„åˆ—å‘é‡ï¼Œ$X^TX$çš„ç‰¹å¾å‘é‡æ˜¯**V**çš„åˆ—å‘é‡ï¼Œ$XX^T$å’Œ$X^TX$æ‹¥æœ‰nç›¸åŒçš„ç‰¹å¾å€¼ï¼Œä¸ºå¥‡å¼‚å€¼ï¼ˆ**S**ä¸»å¯¹è§’çº¿ï¼‰çš„å¹³æ–¹ã€‚

     åœ¨**U**ä¸­é€‰kä¸ªåˆ—å‘é‡$U_{reduce}=[u^{(1)}, ..., u^{(k)}]\in\R^{n\times k}$

  3. $Z=\begin{bmatrix}[u^{(1)}\\ \vdots\\ u^{(k)}]\end{bmatrix}X\\k\times 1\quad k\times n \quad n \times 1$è¿™æ ·å°±æŠŠxï¼ˆnç»´ï¼‰æ˜ å°„åˆ°äº†kç»´ä¸Šã€‚



   * reconstruction

     $X_{approx} = U_{reduce}Z\\n\times 1\quad n\times k\quad k\times 1$

* å¦‚ä½•é€‰æ‹©k

  1. $\dfrac{\dfrac{1}{m}\sum_{i=1}^m\|x^{(i)}-x_{approx}^{(i)}\|^2}{\dfrac{1}{m}\sum_{i=1}^m\|x^{(i)}\|^2}\le0.01/0.05/0.1\cdots$æ˜¯projection error/variationï¼Œè¯´æ˜$99\%/95\%/90\%\cdots$çš„æ–¹å·®è¢«ä¿ç•™ã€‚
  2. $\dfrac{\sum_{i=1}^ks_{ii}}{\sum_{i=1}^n s_{ii}} \ge 0.99/\cdots,\text{from k=1, increse k,}$æœ€å°æ»¡è¶³æ¡ä»¶çš„é‚£ä¸ªkå€¼ï¼Œå°±æ˜¯è¦é€‰æ‹©çš„k

* åº”ç”¨ï¼š

  * æ•°æ®å‹ç¼©ï¼š

    (1) ç›‘ç£å­¦ä¹ åŠ é€Ÿ

    (2) å‡å°‘å­˜å‚¨æ•°æ®æ‰€éœ€çš„å­˜å‚¨å™¨æˆ–ç¡¬ç›˜

  * å¯è§†åŒ–

==**Attention**==ï¼š<u>åœ¨è®¾è®¡ç®—æ³•æ—¶ï¼Œé¦–å…ˆè€ƒè™‘ç”¨raw data</u>ï¼Œåœ¨ç¡®å®å­˜åœ¨é—®é¢˜æ—¶ï¼Œå†è€ƒè™‘ç”¨PCA

